{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bsL77sjx82q"
   },
   "outputs": [],
   "source": [
    "# Data Management\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "# Extras\n",
    "import math, string, os\n",
    "from string import punctuation\n",
    "import re\n",
    "import math\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3304,
     "status": "ok",
     "timestamp": 1573677854629,
     "user": {
      "displayName": "John Tamanas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mABp-m9YfZCzctP0xca2CfrB2TO3hQuLJnqic7FQQ=s64",
      "userId": "04449951801404118058"
     },
     "user_tz": 480
    },
    "id": "B4d-b_SVLedv",
    "outputId": "b25e8758-08e1-4bb3-9d2f-dfb2dbed2854"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/maddocbf/ML/NFL/train.csv' does not exist: b'/Users/maddocbf/ML/NFL/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-082c90054d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/ML/NFL/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1904\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/maddocbf/ML/NFL/train.csv' does not exist: b'/Users/maddocbf/ML/NFL/train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('~/ML/NFL/train.csv')\n",
    "df = df.iloc[0:8800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQkFW9MjFf8n"
   },
   "source": [
    "# Big Ass Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WaUTb-qFyBfB"
   },
   "source": [
    "drive.mount('/content/gdrive')\n",
    "df = pd.read_csv('/content/gdrive/My Drive/nfl/GZUZ/Data Clean/train.csv')\n",
    "df = df.iloc[0: 8800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aogw_MAIwUGr"
   },
   "outputs": [],
   "source": [
    "def clean_plus_features(df):\n",
    "  beginTime = time.time()\n",
    "\n",
    "  for col in df.columns: \n",
    "    if df[col].dtypes == 'O':\n",
    "      df[col].fillna(\"N/A\", inplace=True)\n",
    "    else: \n",
    "      df[col].fillna(df[col].mean(), inplace = True) \n",
    "  def clean_abbrevs(txt):\n",
    "    if 'CLV' == txt:\n",
    "      return 'CLE'\n",
    "    elif 'BLT' == txt:\n",
    "      return 'BAL'\n",
    "    elif 'ARZ' == txt:\n",
    "      return 'ARI'\n",
    "    elif 'HST' == txt:\n",
    "      return 'HOU'\n",
    "    else: \n",
    "      return txt\n",
    "\n",
    "\n",
    "  #StadiumDict = {'out': 'outdoor', 'open':'outdoor', 'in':'indoor','closed':'indoor', 'dome':'rtr roof'}\n",
    "  def clean_StadiumType(txt):\n",
    "    if pd.isna(txt):\n",
    "          return 'N/A'\n",
    "    txt = txt.lower()\n",
    "    if 'out' in txt:\n",
    "      return 'outdoor'\n",
    "    elif 'open' in txt:\n",
    "      return 'outdoor'\n",
    "    elif 'in' in txt:\n",
    "      return 'indoor'\n",
    "    elif 'closed' in txt:\n",
    "      return 'indoor'\n",
    "    elif 'dome' in txt:\n",
    "      return 'rtr roof'\n",
    "    else:\n",
    "      return 'N/A'\n",
    "\n",
    "\n",
    "  def clean_Position(txt):\n",
    "    if txt == 'S':\n",
    "      txt = txt.replace('S', 'SA')\n",
    "    if txt == 'SAF':\n",
    "      txt = txt.replace('SAF', 'SA')\n",
    "    if txt == 'HB':\n",
    "      txt = txt.replace('HB', 'RB')\n",
    "    if txt == 'FB':\n",
    "      txt = txt.replace('FB', 'RB')\n",
    "    if txt == 'MLB':\n",
    "      txt = txt.replace('MLB', \"ILB\")\n",
    "    if txt == 'NT':\n",
    "      txt = txt.replace('NT', 'DT')\n",
    "    if txt =='OG': \n",
    "      txt = txt.replace('OG', 'G')\n",
    "    if txt =='OT': \n",
    "      txt = txt.replace('OT', 'T')\n",
    "    return txt\n",
    "\n",
    "\n",
    "  def clean_GameWeather(txt):\n",
    "    if pd.isna(txt):\n",
    "          return 'N/A'\n",
    "    txt = txt.lower()\n",
    "    if 'rain' in txt:\n",
    "      return 'rain'\n",
    "    elif 'snow' in txt:\n",
    "      return 'snow'\n",
    "    elif 'cloudy' in txt:\n",
    "      return 'cloudy'\n",
    "    elif 'sun' in txt:\n",
    "      return 'clear'\n",
    "    elif 'clear' in txt:\n",
    "      return 'clear'\n",
    "    elif 'in' in txt:\n",
    "      return 'clear'\n",
    "    else:\n",
    "      return 'N/A'\n",
    "\n",
    "\n",
    "  directions = ['n','nne', 'ne', 'ene', 'e', 'ese', 'se', 'sse', 's', 'ssw', 'sw', 'wsw', 'w', 'wnw', 'nw', 'nnw']\n",
    "  def clean_WindDirection(txt):\n",
    "    if pd.isna(txt):\n",
    "          return 'N/A'\n",
    "    txt = txt.lower()\n",
    "    if txt in directions:\n",
    "      return txt\n",
    "    elif 'southwest' in txt:\n",
    "      return 'sw'\n",
    "    elif 'southeast' in txt:\n",
    "      return 'se'\n",
    "    elif 'northwest' in txt:\n",
    "      return 'nw'\n",
    "    elif 'northeast' in txt:\n",
    "      return 'ne'\n",
    "    elif 'north' in txt:\n",
    "      return 'n'\n",
    "    elif 'south' in txt:\n",
    "      return 's'\n",
    "    elif 'west' in txt:\n",
    "      return 'w'\n",
    "    elif 'east' in txt:\n",
    "      return 'e'\n",
    "    elif 'from' in txt:\n",
    "      txt = txt.split(\" \")\n",
    "      return txt[1]\n",
    "    elif '-' in txt:\n",
    "      txt = txt.split(\"-\")\n",
    "      return txt[0]+txt[1]\n",
    "    elif 'calm' in txt:\n",
    "      return 'calm'\n",
    "    else:\n",
    "      return 'N/A'\n",
    "\n",
    "\n",
    "  def clean_Turf(txt):\n",
    "      if pd.isna(txt):\n",
    "          return np.nan\n",
    "      txt = txt.lower()\n",
    "      if 'DD' in txt:\n",
    "        return 'Hybrid'\n",
    "      elif 'SIS' in txt:\n",
    "        return 'Hybrid'\n",
    "      elif 'grass' in txt:\n",
    "        return 'Grass'\n",
    "      elif 'natural' in txt:\n",
    "        return 'Grass'\n",
    "      else:\n",
    "        return 'Turf'\n",
    "\n",
    "  def convert_heights(txt):\n",
    "    ft, inch = txt.split(\"-\")\n",
    "    return int(ft)*12 + int(inch)\n",
    "\n",
    "  def convert_birthday(txt):\n",
    "    month, day, year = txt.split('/')\n",
    "    return ((int(month) - 1)*30 + int(day) + (2019-int(year))*365)/365\n",
    "\n",
    "\n",
    "  def convert_time(txt1, txt2):\n",
    "    date, time = txt1.split('T')\n",
    "    time, z = time.split('Z')\n",
    "    hours, mins, secs = time.split(':')\n",
    "\n",
    "    date2, time2 = txt2.split('T')\n",
    "    time2, z2 = time2.split('Z')\n",
    "    hours2, mins2, secs2 = time2.split(':')\n",
    "\n",
    "    dhours = float(hours2) - float(hours)\n",
    "    dmins = float(mins2) - float(mins)\n",
    "    dsecs = float(secs2) - float(secs)\n",
    "    dtime = dhours * 3600 + dmins * 60 + dsecs\n",
    "    if dtime < 0:\n",
    "      return 86400+dtime\n",
    "    else: \n",
    "      return dtime\n",
    "\n",
    "  def convert_yardline(poss_team, side, yards):\n",
    "    try: \n",
    "      if side == poss_team:\n",
    "        return int(yards)\n",
    "      else:\n",
    "        return -50+int(yards)\n",
    "    except: \n",
    "      return 50.0\n",
    "    \n",
    "\n",
    "  def convert_time_on_clock(quarter, game_clock):\n",
    "    minutes, seconds, a = game_clock.split(':')\n",
    "    time_remaining = (int(quarter))*15*60-int(minutes)-60+int(seconds)\n",
    "    return time_remaining\n",
    "\n",
    "  def windspeed_fix(txt):\n",
    "    txt = str(txt)\n",
    "    try: \n",
    "      speed = float(txt.split(' ')[0])\n",
    "      return speed\n",
    "    except: \n",
    "      return 0.0\n",
    "\n",
    "  def direction_fix(df):\n",
    "    if df.PlayDirection.iloc[0] == 'left':\n",
    "      df.X = 120 - df.X\n",
    "      df.Y = 53.33333 - df.Y\n",
    "      df.Orientation = (180 + df.Orientation)%360.\n",
    "      df.Dir = (180 + df.Dir)%360.\n",
    "    return df\n",
    "\n",
    "  df['PossessionTeam'] = df['PossessionTeam'].apply(clean_abbrevs)\n",
    "  df['FieldPosition'] = df['FieldPosition'].apply(clean_abbrevs)\n",
    "  df['Turf'] = df['Turf'].apply(clean_Turf)\n",
    "  df['WindDirection'] = df['WindDirection'].apply(clean_WindDirection)\n",
    "  df['GameWeather'] = df['GameWeather'].apply(clean_GameWeather)\n",
    "  df['StadiumType'] = df['StadiumType'].apply(clean_StadiumType)\n",
    "  df.loc[df.Season == 2017, 'Orientation'] = np.mod(90 + df.loc[df.Season == 2017, 'Orientation'], 360)\n",
    "  df[\"PlayerHeight\"] = df[\"PlayerHeight\"].apply(convert_heights)\n",
    "  df[\"WindSpeed\"] = df[\"WindSpeed\"].apply(windspeed_fix)\n",
    "  df['PlayerAge'] = df['PlayerBirthDate'].apply(convert_birthday)\n",
    "  df['Position'] = df['Position'].apply(clean_Position)\n",
    "\n",
    "  array=[]\n",
    "  for i in range(df.shape[0]): \n",
    "    array.append(convert_time(df['TimeSnap'][i],df['TimeHandoff'][i]))\n",
    "  df['TimeToHandoff'] = array\n",
    "\n",
    "  array=[]\n",
    "  for i in range(df.shape[0]): \n",
    "    array.append(convert_yardline(df['PossessionTeam'][i], df['FieldPosition'][i],df['YardLine'][i]))\n",
    "  df['ScrimmageLine'] = array\n",
    "\n",
    "  array=[]\n",
    "  for i in range(df.shape[0]): \n",
    "    array.append(convert_time_on_clock(df['Quarter'][i], df['GameClock'][i]))\n",
    "  df['Time'] = array\n",
    "\n",
    "  df = df.groupby(\"PlayId\", group_keys=False).apply(direction_fix)\n",
    "  basicCleanTime = time.time()\n",
    "\n",
    "\n",
    "  def add_positions(df):\n",
    "    off_pos = ''.join(df.OffensePersonnel.unique())\n",
    "    off_pos = np.unique(re.findall(r'\\b[A-Z][A-Z]+\\b', off_pos))\n",
    "    off_pos = ['Off' + p for p in off_pos]\n",
    "    \n",
    "    def_pos = ''.join(df.DefensePersonnel.unique())\n",
    "    def_pos = np.unique(re.findall(r'\\b[A-Z][A-Z]+\\b', def_pos))\n",
    "    def_pos = ['Def' + p for p in def_pos]\n",
    "  \n",
    "    df = df.reindex(columns=df.columns.values.tolist() + off_pos + def_pos)\n",
    "  \n",
    "    def parse_df(df):\n",
    "      off_ = df.iloc[0].OffensePersonnel.replace(',', ' ').split()\n",
    "      nums, cols = off_[::2], off_[1::2]\n",
    "      cols = ['Off' + c for c in cols]\n",
    "      df[cols] = nums\n",
    "      \n",
    "      def_ = df.iloc[0].DefensePersonnel.replace(',', ' ').split()\n",
    "      nums, cols = def_[::2], def_[1::2]\n",
    "      cols = ['Def' + c for c in cols]\n",
    "      df[cols] = nums\n",
    "      return df\n",
    "    \n",
    "    df = df.groupby('PlayId').apply(parse_df)\n",
    "    df.drop(columns=['OffensePersonnel', 'DefensePersonnel'], inplace=True)\n",
    "\n",
    "    for c in off_pos + def_pos:\n",
    "      df[c] = df[c].fillna(0)\n",
    "      \n",
    "    return df\n",
    "\n",
    "  df = add_positions(df)\n",
    "\n",
    "  positionTime = time.time()\n",
    "  def make_vectors(points):\n",
    "    \"\"\"\n",
    "    Take a collection of vertices (points) of a convex shape\n",
    "    and create a set of vectors from the uppermost point\n",
    "    to the rest of the points\n",
    "    \"\"\"\n",
    "    points = np.array(sorted(points, key=lambda x: -x[1]))\n",
    "    vecs = (points - points[0])[1:]\n",
    "    return vecs\n",
    "\n",
    "  def order_vectors(vectors):\n",
    "      \"\"\"\n",
    "      top: top of polygon from which all vectors are defined\n",
    "      vectors: (x, y) of vectors defined by make_vectors\n",
    "      \"\"\"\n",
    "      def angle(vec):\n",
    "          xhat = np.array([1, 0])\n",
    "          sin_angle = np.linalg.norm(np.cross(xhat, vec))/np.linalg.norm(vec)\n",
    "          angle = np.arcsin(sin_angle)\n",
    "          if vec[0] < 0:\n",
    "              # because arcsin only gives between -pi/2 and pi/2\n",
    "              angle = np.pi - angle\n",
    "          return angle\n",
    "      \n",
    "      return sorted(vectors, key=lambda vec: angle(vec))\n",
    "\n",
    "  def polygon_area(points):\n",
    "      \"\"\"\n",
    "      We break the polygon into triangles and then sum up\n",
    "      the areas of those individual triangles to determine\n",
    "      the total area of the polygon. This only works because\n",
    "      the polygons are convex\n",
    "      \"\"\"\n",
    "      vectors = make_vectors(points)\n",
    "      ord_vecs = order_vectors(vectors)\n",
    "      \n",
    "      area = 0\n",
    "      i = 0\n",
    "      while i < len(ord_vecs)-1:\n",
    "          area += 0.5 * np.linalg.norm(np.cross(ord_vecs[i], ord_vecs[i+1]))\n",
    "          i += 1\n",
    "      \n",
    "      return area\n",
    "\n",
    "  def get_dx_dy(delta_t, rusher_dir, rusher_or, speed, acceleration):\n",
    "      dx = (delta_t*speed) * math.cos(rusher_dir)\n",
    "      dx = dx + 0.5*delta_t**2 * acceleration * math.cos(rusher_or)\n",
    "      dy = (delta_t*speed) * math.sin(rusher_dir)\n",
    "      dy = dy + 0.5*delta_t**2 * acceleration * math.sin(rusher_or)\n",
    "      return dx, dy\n",
    "\n",
    "  def voronoi_dist_to_rb(df, delta_t):\n",
    "        \"\"\"Compute Voronoi areas and distance to RB for an array of Δt offsets.\"\"\"\n",
    "        try:\n",
    "          iter(delta_t)\n",
    "        except:\n",
    "          delta_t = [delta_t]\n",
    "\n",
    "        direction = np.mod(90. - df.Dir, 360.) * np.pi / 180.\n",
    "        orientation = np.mod(90. - df.Orientation, 360.) * np.pi / 180.\n",
    "\n",
    "        for dt in delta_t:\n",
    "          dx = (dt * df.S) * np.cos(direction) + 0.5 * dt ** 2 * df.A * np.cos(orientation)\n",
    "          dy = (dt * df.S) * np.sin(direction) + 0.5 * dt ** 2 * df.A * np.sin(orientation)\n",
    "          x_ = df.X + dx; y_ = df.Y + dy\n",
    "          df['X_' + str(dt)] = x_\n",
    "          df['Y_' + str(dt)] = y_\n",
    "\n",
    "        def add_features(df):\n",
    "          for dt in delta_t:\n",
    "              # Compute Voronoi areas\n",
    "              coords = ['X_' + str(dt), 'Y_' + str(dt)]\n",
    "              vor = Voronoi(df[coords].values)\n",
    "              areas = np.array([polygon_area([vor.vertices[i] for i in region])\n",
    "                              if len(region) > 0 else 0. for region in vor.regions])\n",
    "              df['VoronoiArea_' + str(dt)] = areas[vor.point_region]\n",
    "\n",
    "              # Compute distance to RB\n",
    "              rusher = df[df.NflId == df.NflIdRusher]\n",
    "              dist = np.linalg.norm((df[coords] - rusher[coords].values).values, axis=1)\n",
    "              df['DistanceToRB_dt_' + str(dt)] = dist\n",
    "          return df\n",
    "\n",
    "        df = df.groupby('PlayId', group_keys=False).apply(add_features)\n",
    "        df = df.drop(columns=['X_' + str(dt) for dt in delta_t] + ['Y_' + str(dt) for dt in delta_t])\n",
    "        return df\n",
    "\n",
    "  delta_t = [0., 0.25, 0.5, 0.75]\n",
    "  #df = voronoi_dist_to_rb(df, delta_t)\n",
    "  voronoiTime = time.time()\n",
    "\n",
    "  def centroid_dif(df): \n",
    "    homeTeam = df[df.Team == 'home']\n",
    "    awayTeam = df[df.Team == 'away']\n",
    "    centroid = np.sqrt((homeTeam.X.mean()-awayTeam.X.mean())* (homeTeam.X.mean()-awayTeam.X.mean()) \n",
    "                    +(homeTeam.Y.mean()-awayTeam.Y.mean())*(homeTeam.Y.mean()-awayTeam.Y.mean()))\n",
    "    df[\"Centroid\"] = np.ones((22,))*centroid\n",
    "    return df\n",
    "\n",
    "  centroidTime = time.time()\n",
    "\n",
    "  df = df.groupby('PlayId', group_keys=False).apply(centroid_dif)\n",
    "\n",
    "  df['MinTimeToTackle']= df['DistanceToRB_dt_0.0']/df.S\n",
    "\n",
    "  direction = np.mod(90. - df.Dir, 360.) * np.pi / 180.0\n",
    "  orientation = np.mod(90. - df.Orientation, 360.) * np.pi / 180.0\n",
    "  df['Sx'] =  (df.S) * np.cos(direction)\n",
    "  df['Sy'] = (df.S) * np.sin(direction)\n",
    "\n",
    "\n",
    "  def nn_to_rb(df):\n",
    "    \"\"\"Get nearest neighbors to running back\"\"\"\n",
    "    is_defense = (~np.any(df.NflId == df.NflIdRusher)).astype('int')\n",
    "    df = df.sort_values('DistanceToRB_dt_0.0')\n",
    "    df['NNToRB'] = is_defense * 11 + np.arange(11)\n",
    "    return df\n",
    "\n",
    "\n",
    "  df = df.groupby(['PlayId', 'Team'], group_keys=False).apply(nn_to_rb)\n",
    "  nnTime = time.time()\n",
    "  df.to_csv('/content/gdrive/My Drive/nfl/GZUZ/test_B.csv', index=False)\n",
    "\n",
    "\n",
    "  def flatten_data(df): \n",
    "    \"\"\"Flatten grouped dataframe and return that dataframe\"\"\" \n",
    "    df = df.sort_values(by = ['NNToRB'])\n",
    "    array = df.values.flatten()\n",
    "    array = np.transpose(array.reshape(array.shape[0], 1))\n",
    "    df = pd.DataFrame(data = array, columns = playerDataCol)\n",
    "    return df\n",
    "\n",
    "  def game_data(df):\n",
    "    df = df.iloc[0,:]\n",
    "    array = df.values.flatten()\n",
    "    array = np.transpose(array.reshape(array.shape[0], 1))\n",
    "    df = pd.DataFrame(data = array, columns = gameData)\n",
    "    return df\n",
    "\n",
    "  \"\"\"We need a better way of picking out the columns related to players vs. games \n",
    "    although I guess doing it manually isn't that bad.... \"\"\"\n",
    "\n",
    "  gameData = ['GameId', 'PlayId', 'Team', 'Season', 'YardLine',\n",
    "        'Quarter', 'GameClock', 'PossessionTeam', 'Down', 'Distance',\n",
    "        'FieldPosition', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay',\n",
    "        'NflIdRusher', 'OffenseFormation', 'DefendersInTheBox', 'PlayDirection',\n",
    "        'TimeHandoff', 'TimeSnap', 'Yards', 'HomeTeamAbbr',\n",
    "        'VisitorTeamAbbr', 'Week', 'Stadium', 'Location', 'StadiumType', 'Turf',\n",
    "        'GameWeather', 'Temperature', 'Humidity', 'WindSpeed', 'WindDirection',\n",
    "        'TimeToHandoff', 'ScrimmageLine', 'Time', 'OffOL',\n",
    "        'OffRB', 'OffTE', 'OffWR', 'DefDB', 'DefDL', 'DefLB']\n",
    "\n",
    "  playerData=['PlayId','X', 'Y', 'S', 'A', 'Dis', 'Orientation',\n",
    "        'Dir', 'NflId', 'PlayerHeight', 'PlayerWeight', 'Position', \n",
    "        'PlayerAge',\n",
    "        'DistanceToRB_dt_0.0', 'DistanceToRB_dt_0.25', 'DistanceToRB_dt_0.5',\n",
    "        'DistanceToRB_dt_0.75','VoronoiArea_0.0','VoronoiArea_0.25','VoronoiArea_0.5','VoronoiArea_0.75',  'NNToRB']\n",
    "        \n",
    "\n",
    "\n",
    "  playerDataCol=[]\n",
    "  sides = ['_O_', '_D_']\n",
    "  for k,side in enumerate(sides): \n",
    "    for i in range(11): \n",
    "      for player_values in playerData: \n",
    "        title = str(player_values)+side+str(i+k)\n",
    "        playerDataCol.append(title)\n",
    "\n",
    "  dfPlayers = df[playerData]\n",
    "  dfGame = df[gameData]\n",
    "\n",
    "  dfPlayers = dfPlayers.groupby(['PlayId'], group_keys=False).apply(flatten_data)\n",
    "  dfGame = dfGame.groupby(['PlayId'], group_keys=False).apply(game_data)\n",
    "\n",
    "  df = pd.concat([dfGame, dfPlayers], axis = 1)\n",
    "  #print(df2.X_O_0.value_counts(), df2.Distance.value_counts(), df2.)\n",
    "  df['HandoffToFD'] = df['Distance'] + (df['X_O_0']-df['ScrimmageLine'])\n",
    "\n",
    "  df.to_csv('/content/gdrive/My Drive/nfl/GZUZ/test_A.csv', index=False)\n",
    "  flatTime = time.time()\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Pu5IzNzFk7T"
   },
   "source": [
    "# End of Big Ass Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46939,
     "status": "ok",
     "timestamp": 1573677902448,
     "user": {
      "displayName": "John Tamanas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mABp-m9YfZCzctP0xca2CfrB2TO3hQuLJnqic7FQQ=s64",
      "userId": "04449951801404118058"
     },
     "user_tz": 480
    },
    "id": "Egxp7S1_yXtX",
    "outputId": "e5a3142b-e1fb-41a8-c882-cd676a8463d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:271: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.5283637046814\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "df = clean_plus_features(df)\n",
    "time2=time.time()\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y-EK9sHsMCC"
   },
   "outputs": [],
   "source": [
    "df.to_csv('/content/gdrive/My Drive/nfl/GZUZ/FinalSetTest.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pQkFW9MjFf8n"
   ],
   "name": "CleanSingleFunction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
